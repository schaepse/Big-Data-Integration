---
title: "Big Data Integration Final Paper"
author: "Sidney Schaeper, Aman Rastogi, Keya Satpathy, Bhawna Saini, Prathiba Swamykannu"
date: "2/20/2020"
output:  word_document
---
### Understanding Our Topic
We had a variety of questions we wanted to answer in this project. We formualted these questions after evalauting our 84.51 datasets and the twitter data. The following are the questions we hoped to answer. 

* Should we continue to do business with our five worst performing brands based on the perspective of customers?
* Does location of the product within the store and ad impact sales of the product? What is the optimal location within the store and ads?
* What amounts of each product should we plan to have in inventory on average daily based on these sales?  

For the first question, we thought that the twitter dataset, product_lookup dataset, and transactions dataset would provide us insight into answering this question. For the second question, we thought that the transactions dataset and casual_lookup dataset would provide us information into answering this question. For the third question, we thought that the transactions dataset, store_lookup dataset, and product_lookup dataset would help us discover the answer to this question. 

We thought the first question would be insightful to the business, because a company should know what customers think of the brands they partner with. A company likely wouldn't want to work with a brand that has a bad image to its customers, because they likely wouldn't sell many of the products associated with that brand. In addition, we considered the second question to be a great question to answer, because we thought this could help Kroger's marketing strategy. The marketing strategy would be improved with this knowledge, because the marketers would have more knowledge of the optimal locations for high product sales in the weekly ad and stores. The final question would be great for a busines to know, because this knowledge would help improve a company's inventory strategy. This strategy would improve, because you possibly could improve your inventory prediction levels for each store from using the data instead of just guessing. 

### Describe The Tools Used
The tool that we used for this project was R Studio.

```{r eval = FALSE}
#Packages that needed to be installed for this project#
install.packages("readr")
install.packages("dplyr")
install.packages("tidyr")
install.packages("shiny")
install.packages("DT")
install.packages("ggplot2")
install.packages("plotly")
install.packages("tidyverse")
install.packages("tm")
install.packages("ggwordcloud")
install.packages("tidyr")
```

```{r eval = FALSE}
#Packages that we needed for the presentation but can't have in the paper due to these being interactive#
library(shiny)
library(DT)
```

```{r warning = FALSE, message = FALSE}
#Packages that need to be found for this project for the paper#
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(plotly)
library(tidyverse)
library(tm)
library(ggwordcloud)
library(tidyr)
```

### 84.51 Data Source
The data that we wanted to evaluate for this project came from two sources. The first source was from [84.51's website](https://www.8451.com/). The folder of data that we used from 84.51 are called the Carbo Loading datasets. There are a total of four datasets within this folder. The four datasets are called the casual_lookup, product_lookup, store_lookup, and transactions. According to the Carbo Loading guide provided, these datasets came from a relational database. Also, these datasets contain purchases at a household level over two years, and the datasets are filtered to only contain products from four categories. These four categories are pasta, pasta sauce, syrup, and pancake mix. In addition, the guide contained information pertaining to the variables within each of the datasets. The tables below show these variables and their description. 

**Transactions Dataset Table**

Variable | Description 
---------|------------
upc | It is a standard 10 digit code assigned to products. This is the product that was purchased.
dollar_sales | The amount of money spent on this product by the customer. These are recorded in dollars.
units | The quantity of this product purchased. 
time_of_transaction | The time the transaction occurred. This is recorded in military time. 
geography | This label tells you where it was purchased out of the two large regions. These two regions consist of multiple states. The value can either be 1 or 2. 
week | This notifies the week that the transaction occured. The range of values is from 1 to 104. These numbers are assigned chronologically. 
household | This value is a unique number assigned to a household. This is the purchaser of the product.
store | This value is a unique number assigned to each store. This is where the product was purchased.
basket | This is a unique number assigned to a trip to the store. This is the trip that this product was assigned to. 
day | This is the day that this product was purchased. The range of values is 1 to 728. 
coupon | This is dummy variable to notify whether a coupon was used. The possible value is 1 or 0. 1 means a coupon was used. 0 means a coupon wasn't used. 

**Store Lookup Dataset Table**

Variable | Description 
---------|------------
store | This value is a unique number assigned to each store.
store_zip_code | This is the 5 digit zip code for the store. 

**Product Lookup Dataset Table**

Variable | Description
---------|------------
upc | This is the standard 10 digit code assigned to this product. 
product_description | This details the product. This likely contains the name of the product.
commodity | This is the category of the product. The four possibly values are pasta, pasta sauce, pancake mix, or syrup. 
brand | This is the brand name of the product. 
product_size | This is the size of the product. These aren't all in the same measurement. 

**Casual Lookup Dataset Table**

Variable | Description
---------|------------
upc | This is the standard 10 digit code assigned to this product. 
store | This value is a unique number assigned to each store.
week | This notifies the week that the transaction occured. The range of values is from 1 to 104. These numbers are assigned chronologically. 
feature_desc | This is where the product is located on the weekly ad. 
display_desc | This is where the product is displayed in the store. 
geography | This label tells you where it was purchased out of the two large regions. These two regions consist of multiple states. The value can either be 1 or 2. 

```{r eval = TRUE, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
#Set the working directory#
setwd("~/8451_Carbo-Loading/Carbo-Loading CSV")

#Read the datasets into R#
causal <- read_csv("causal_lookup.csv")
transaction <- read_csv("transactions.csv")
product <-read_csv("product_lookup.csv")
store <- read_csv("store_lookup.csv")
```

```{r}
#View snippets of the dataset#
head(causal)
head(transaction)
head(product)
head(store)
```

### Twitter Data Source
The final source of data we used for our projet was from [Twitter](http://apps.twitter.com). We had to create a developer account in order to get access to these tweets. In this project, we were hoping to pull this data manually to get customer opinions about brands. The way we pulled these tweets down manually was through using the search tweets package in R, and we searched through twitter to find these tweets by searching on the brand name and the food category of the brand. Next, we cleaned these results by focusing just on the tweet, removing emojis, removing urls, lowering the case of the tweets, removing the punctation, removing the numbers, removing stop words, removing white space, and removing additional words that we don't want to evaluate. After we cleaned the tweets, we separated the tweets into words, counted these words, and filtered on the top 25 words. After collecting these results, we created a data frame from all these results. If a brand and category didn't show a result, we put NA for these word results. Below is a table of the dataset we created manually, and this table shows the variables and description. 

**Twitter Dataset Table**

Variable | Description 
---------|------------
brand | This is the name of the brand. 
word | These are the words that showed up in the top 25 filter.
freq | These are the counts of the word showing up in the tweets.

### Zip Data Source

**Zip Dataset Table**

Variable | Description
---------|------------
zip| zip code

```{r eval = TRUE, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
#Set the working directory#
setwd("~/8451_Carbo-Loading/Carbo-Loading CSV")

#Read the zip dataset into R#
zip <- read_csv("zip.csv")
```

```{r}
#See a partial view of the dataset#
head(zip)
```

### Data Integration Process 
```{r}
#Clean the files before the inner join for Question 2#
causal_select <- causal %>%
  select(upc, store, week, feature_desc, display_desc)
transaction_clean <- transaction %>%
  filter(week > 42) %>%
  select(upc, dollar_sales, units, week, store)

#Inner join for Question 2#
causal_trans <- inner_join(causal_select, transaction_clean, by = c("upc", "store", "week"))
tibble(causal_trans)
```

```{r}
#Make some transformations of the data for Question 3#
product_transform <- transform(product, upc = as.character(upc))
store_transform <- transform(store, store_zip_code = as.character(store_zip_code))
names(store_transform)[2] <- "Zip"
zip_transform <- transform(zip, Zip = as.character(Zip))
```

```{r}
#Select the columns needed from each of the datasets for Question 3#
transaction_select <- transaction %>%
  select(upc, units, week, store, day)
product_select <- product_transform %>%
  select(upc, product_description)
store_select <- store_transform %>%
  select(store, Zip)

#Inner join on all these datasets for Question 3#
transaction_product <- inner_join(transaction_select, product_select, by = "upc")
head(transaction_product)  
transaction_product_store <- inner_join(transaction_product, store_select, by = "store")
head(transaction_product_store)
```

```{r}
#Additional inner join for question 3 for creating the zip code map#
#Group the data by zip code and the sum the quanity of products sold in these zip codes#
head(transaction_product_store)
zip_map_data_2 <- transaction_product_store %>%
  group_by(Zip) %>%
  summarise(Total_Quantity_Zip = sum(units, na.rm = TRUE))

#Inner join on zip and transaction product store#
transaction_product_store_zip <- inner_join(zip_map_data_2, zip_transform, by = "Zip")
head(transaction_product_store_zip)
```

### Schema Alignment 
#### Record Linkage 
#### Data Fusion
### Final Results and Analyzing These Results 
```{r}
#Editing of the Combined Dataset to Find the Answers for Question 2#
#Feature Description#
causal_trans_feature_dollar <- causal_trans %>%
  select(feature_desc, dollar_sales)%>%
  group_by(feature_desc) %>%
  summarise(Total_Dollar_Sales = sum(dollar_sales, na.rm = TRUE)) %>%
  arrange(desc(Total_Dollar_Sales))
causal_trans_feature_dollar

causal_trans_feature_units <- causal_trans %>%
  select(feature_desc, units)%>%
  group_by(feature_desc) %>%
  summarise(Total_Units = sum(units, na.rm = TRUE)) %>%
  arrange(desc(Total_Units))
causal_trans_feature_units

causal_trans_feature_units_dollar <- causal_trans %>%
  select(feature_desc, units, dollar_sales)%>%
  group_by(feature_desc) %>%
  summarise(Sales_Divided_By_Units = sum(dollar_sales, na.rm = TRUE)/sum(units, na.rm = TRUE)) %>%
  arrange(desc(Sales_Divided_By_Units))
causal_trans_feature_units_dollar

#Display_desc#
causal_trans_display_dollar <- causal_trans %>%
  select(display_desc, dollar_sales)%>%
  group_by(display_desc) %>%
  summarise(Total_Dollar_Sales = sum(dollar_sales, na.rm = TRUE)) %>%
  arrange(desc(Total_Dollar_Sales))
causal_trans_display_dollar

causal_trans_display_units <- causal_trans %>%
  select(display_desc, units)%>%
  group_by(display_desc) %>%
  summarise(Total_Units = sum(units, na.rm = TRUE)) %>%
  arrange(desc(Total_Units))
causal_trans_display_units

causal_trans_display_units_dollar <- causal_trans %>%
  select(display_desc, units, dollar_sales)%>%
  group_by(display_desc) %>%
  summarise(Sales_Divided_By_Units = sum(dollar_sales, na.rm = TRUE)/sum(units, na.rm = TRUE)) %>%
  arrange(desc(Sales_Divided_By_Units))
causal_trans_display_units_dollar
```

```{r}
#Code to get Answers for Question 3#
#Part 1#
#Group the data by day and product description and create a calculation to sum the units by day#
combine_day_sum <- transaction_product_store %>%
  group_by(day, product_description) %>%
  summarise(Sum_Quantity_by_Day = sum(units, na.rm = TRUE))
#Preview of dataset for paper#
head(combine_day_sum)
#The shiny table we created to show these results can be found on website for the presentation. The image below is a snippet of it. The code we used to create this table is found in the additional code section of the paper.#
```



```{r}
#Part 2#
#Group the data by day, sum the units of the day, and arrange it by day#
combine_day_time <- transaction_product_store %>%
  group_by(day) %>%
  summarise(Total_Quantity_Day = sum(units, na.rm = TRUE)) %>%
  arrange(day)
#Preview the dataset#
head(combine_day_time)

#Find the top 5 days when products were sold#
top_5_days <- combine_day_time %>%
  arrange(desc(Total_Quantity_Day)) %>%
  head(5)
top_5_days

#Create a line graph showing the quantity sold over time#
ggplot(data = combine_day_time, aes(x = day, y = Total_Quantity_Day, group=1)) +
  geom_line() +
  geom_point() +
  ggtitle("Quantity Over Time", subtitle = "This shows the quantity over all the days recorded in the dataset.")
```

```{r}
#Part 3#
#Select the columns we need for the zip data and arrange it in descending order by Total Quanity Sold#
zip_map_data <- transaction_product_store_zip %>%
  select(Zip, Latitude, Longitude, Total_Quantity_Zip) %>%
  arrange(desc(Total_Quantity_Zip))
#This is a preview of the dataset we used to create the map in our presentation. Also, it shows the Top 5 zip codes for quanity sold#
head(zip_map_data, 5)
#We can't display the map because it is interactive. Below is an image of what the maap looks like. The code we used will be found in the additional code section of the paper#
```

### Future Steps 
### Bibliography 
### Additonal Significant Code 